#import "../template.typ": parec, ez_caption

== Film and Imaging
<film-and-imaging>
#parec[
  After the camera's projection or lens system forms an image of the scene on the film, it is necessary to model how the film measures light to create the final image generated by the renderer. This section starts with an overview of the radiometry of how light is measured on the film and then continues with the topic of how spectral energy is converted to tristimulus colors (typically, RGB). This leads to the #link("<PixelSensor>")[`PixelSensor`] class, which models that process as well as further processing that is generally performed by cameras. After next considering how image samples on the film are accumulated into the pixels of the final image, we introduce the #link("<Film>")[`Film`] interface and then two implementations of it that put this model into practice.
][
  在相机的投影或镜头系统在胶卷上形成场景图像之后，有必要对胶卷如何测量光线以创建由渲染器生成的最终图像进行建模。本节首先概述了胶卷上光线测量的辐射度学，然后继续讨论光谱能量如何转换为三刺激颜色（通常为RGB）。这引出了#link("<PixelSensor>")[`PixelSensor`];类，它不仅对该过程进行建模，还对通常由相机执行的进一步处理进行建模。接下来，我们将考虑胶卷上的图像样本如何累积到最终图像的像素中，然后介绍#link("<Film>")[`Film`];接口及其两个实现，这些实现将此模型付诸实践。
]

=== The Camera Measurement Equation
<the-camera-measurement-equation>


#parec[
  Given a simulation of the process of real image formation, it is also worthwhile to more carefully define the radiometry of the measurement made by a film or a camera sensor. Rays from the rear of the lens to the film carry radiance from the scene. As considered from a point on the film plane, there is thus a set of directions from which radiance is incident. The distribution of radiance leaving the lens is affected by the amount of defocus blur seen by the point on the film—Figure 5.17 shows two images of the radiance from the lens as seen from two points on the film.
][
  给定真实图像形成过程的模拟，还值得更仔细地定义胶卷或相机传感器所做测量的辐射度学。从镜头后部到胶卷的光线携带来自场景的辐射亮度。从胶卷平面上的一个点来看，存在一组辐射亮度入射的方向。离开镜头的辐射亮度的分布受到胶卷上点所见的散焦模糊量的影响——图5.17显示了从胶卷上两个点看到的镜头辐射亮度的两个图像。
]


#figure(
  table(
    columns: 2,
    stroke: none,
    [#image("../pbr-book-website/4ed/Cameras_and_Film/exit-pupil-a.png", width: 50%)],
    [#image("../pbr-book-website/4ed/Cameras_and_Film/exit-pupil-b.png", width: 50%)],
  ),
  caption: [
    #ez_caption[The Image of the Scene on the Lens, as Seen from Two Points on the Film Plane. Both are from a rendering of the San Miguel scene. (a) As seen from a point where the scene is in sharp focus; the incident radiance is effectively constant over its area. (b) As seen from a pixel in an out-of-focus area, a small image of part of the scene is visible, with potentially rapidly varying radiance.][场景在镜头上的图像，从胶卷平面上的两个点观察。两者都来自于圣米格尔场景的渲染。(a) 从一个场景清晰对焦的点观察，入射辐射在该区域内基本保持恒定。(b) 从一个模糊区域的像素观察，场景部分的小图像可见，辐射可能快速变化。]
  ],
  kind: image,
)
#parec[
  Given the incident radiance function, we can define the irradiance at a point on the film plane. If we start with the definition of irradiance in terms of radiance, Equation (4.7), we can then convert from an integral over solid angle to an integral over area (in this case, an area $A_e$ of the plane tangent to the rear lens element) using Equation (4.9). This gives us the irradiance for a point $p$ on the film plane:
][
  给定入射辐射亮度函数，我们可以定义胶卷平面上某一点的辐照度。如果我们从辐照度关于辐射亮度的定义开始，方程(4.7)，然后可以使用方程(4.9)从对立体角的积分转换为面积积分（在这种情况下，是后镜头元件切平面上的面积 $A_e$ ）。这给出了胶卷平面上一点 $p$ 的辐照度：
]

$ E (p) = integral_(A_e) L_i (p , p prime) lr(|cos theta cos theta prime|) / lr(|p prime - p|)^2 d A_e $


#parec[
  Figure 5.18 shows the geometry of the situation.
][
  图5.18展示了这种情况的几何设置。
]

#figure(
  image("../pbr-book-website/4ed/Cameras_and_Film/pha05f18.svg"),
  caption: [
    #ez_caption[Figure 5.18: Geometric setting for the irradiance measurement
      equation, (5.3). Radiance can be measured as it passes through
      points $p prime$ on the plane tangent to the rear lens element to a
      point on the film plane $p$. $z$ is the axial distance from the film
      plane to the rear element tangent plane, and $theta$ is the angle
      between the vector from $p prime$ to $p$ and the optical axis.][
      图 5.18：辐照度测量方程 (5.3) 的几何设置。辐射度可以在其通过后透镜元件切线平面上的点 $p'$ 到胶片平面上的点 $p$ 时测量。$z$ 表示从胶片平面到后元件切线平面的轴向距离，$θ$ 是从 $p'$ 到 $p$ 的向量与光轴之间的角度。
    ]
  ],
)


#parec[
  Because the film plane is parallel to the lens's plane, $theta = theta prime$. We can further take advantage of the fact that the distance between $p$ and $p prime$ is equal to the axial distance from the film plane to the lens (which we will denote here by $z$ ) divided by $cos theta$. Putting this all together, we have:
][
  由于胶卷平面与镜头平面平行， $theta = theta prime$。我们还可以利用 $p$ 和 $p prime$ 之间的距离等于从胶卷平面到镜头的轴向距离（我们将在此处表示为 $z$ ）除以 $cos theta$ 这一事实。综合以上，我们得到：
]

$ E (p) = 1 / z^2 integral_(A_e) L_i (p , p prime) lr(|cos^4 theta|) d A_e $
(5.3)


#parec[
  For cameras where the extent of the film is relatively large with respect to the distance $z$, the $cos^4 theta$ term can meaningfully reduce the incident irradiance—this factor also contributes to vignetting. Most modern digital cameras correct for this effect with preset correction factors that increase pixel values toward the edges of the sensor.
][
  对于胶卷范围相对于距离 $z$ 相对较大的相机， $cos^4 theta$ 项可以显著减少入射辐照度——这一因素也导致渐晕。大多数现代数码相机通过预设校正因子来纠正这种效应，这些因子会增加传感器边缘的像素值。
]

#parec[
  Integrating irradiance at a point on the film over the time that the shutter is open gives #emph[radiant exposure];, which is the radiometric unit for energy per area, $J \/ m^2$.
][
  在快门打开的时间内对胶卷上一点的辐照度进行积分得到#emph[辐射曝光];，这是每单位面积的能量的辐射度单位， $J \/ m^2$。
]

$
  H (p) = 1 / z^2 integral_(t_0)^(t_1) integral_(A_e) L_i (p , p prime , t prime) lr(|cos^4 theta|) d A_e thin d t prime
$
(5.4)


// === Radiant Exposure
// <radiant-exposure>
#parec[
  (Radiant exposure is also known as #emph[fluence];.) Measuring radiant exposure at a point captures the effect that the amount of energy received on the film plane is partially related to the length of time the camera shutter is open.
][
  （辐射曝光也称为#emph[辐射通量];。）在一个点测量辐射曝光能够捕捉到能量接收量与相机快门打开时间长度之间的部分关系。
]

#parec[
  Photographic film (or CCD or CMOS sensors in digital cameras) measures radiant energy over a small area.\[^1\] Taking Equation (5.4) and also integrating over sensor pixel area, $A_p$, we have

  $
    J = 1 / z^2 integral_(A_p) integral_(t_0)^(t_1) integral_(A_e) L_i ( p , p prime , t prime ) lr(|cos^4 theta|) thin d A_e thin d t prime thin d A_p ,
  $ the Joules arriving at a pixel; this is called the #emph[camera
measurement equation];.
][
  摄影胶片（或数字相机中的 CCD 或 CMOS 传感器）用于测量小区域内的辐射能量。\[^1\] 取方程 (5.4) 并在传感器像素区域 $A_p$ 上进行积分，我们得到

  $
    J = 1 / z^2 integral_(A_p) integral_(t_0)^(t_1) integral_(A_e) L_i ( p , p prime , t prime ) lr(|cos^4 theta|) thin d A_e thin d t prime thin d A_p ,
  $ 到达一个像素的焦耳；这被称为#emph[相机测量方程];。
]

#parec[
  Although these factors apply to all of the camera models introduced in this chapter, they are only included in the implementation of the `RealisticCamera`. The reason is purely pragmatic: most renderers do not model this effect, so omitting it from the simpler camera models makes it easier to compare images rendered by `pbrt` with those rendered by other systems.
][
  虽然这些因素适用于本章介绍的所有相机模型，但它们仅包含在 `RealisticCamera` 的实现中。原因完全是出于实用考虑：大多数渲染器不对这种效果建模，因此在较简单的相机模型中省略它可以更容易地将 `pbrt` 渲染的图像与其他系统渲染的图像进行比较。
]

=== Modeling Sensor Response
<modeling-sensor-response>
#parec[
  Traditional film is based on a chemical process where silver halide crystals produce silver bromide when they are exposed to light. Silver halide is mostly sensitive to blue light, but color images can be captured using multiple layers of crystals with color filters between them and dyes that make silver halide more responsive to other wavelengths.
][
  传统胶片基于一种化学过程，其中银卤化物晶体在暴露于光时产生溴化银。银卤化物主要对蓝光敏感，但可以通过在它们之间使用彩色滤光片和使银卤化物对其他波长更敏感的染料的多层晶体来捕捉彩色图像。
]

#parec[
  Modern digital cameras use CCD or CMOS sensors where each pixel effectively counts the number of photons it is exposed to by transforming photons into electrical charge. A variety of approaches to capturing color images have been developed, but the most common of them is to have a color filter over each pixel so that each measures red, green, or blue by counting only the photons that make it through the filter. Each pixel is often supplemented with a #emph[microlens] that increases the amount of light that reaches the sensor.
][
  现代数字相机使用 CCD 或 CMOS 传感器，其中每个像素通过将光子转化为电荷来有效地计数其接收到的光子数量。已经开发了多种捕捉彩色图像的方法，但最常见的是在每个像素上覆盖一个彩色滤光片，以便每个像素通过仅计数通过滤光片的光子来测量红色、绿色或蓝色。每个像素通常配备一个#emph[微透镜];，以增加到达传感器的光量。
]

#parec[
  For both film and digital sensors, color measurements at pixels can be modeled using spectral response curves that characterize the color filter or film's chemical response to light as a function of wavelength. These functions are defined such that, for example, given an incident spectral distribution $s (lambda)$, a pixel's red component is given by
][
  对于胶片和数字传感器，像素的颜色测量可以使用光谱响应曲线进行建模，这些曲线表征了彩色滤光片或胶片对光的化学响应作为波长的函数。这些函数被定义为，例如，给定入射光谱分布 $s (lambda)$，像素的红色分量可以通过以下公式计算
]

$ r = integral s (lambda) overline(r) (lambda) thin d lambda . $

#parec[
  Digital sensor pixels are typically arranged in a #emph[mosaic];, with twice as many green pixels as red and blue, due to the human visual system's greater sensitivity to green. One implication of pixel mosaics is that a #emph[demosaicing] algorithm must be used to convert these sensor pixels to image pixels where the red, green, and blue color components are colocated. The naive approach of taking quads of mosaiced pixels and using their color values as is does not work well, since the constituent sensor pixels are at slightly different locations.
][
  数字传感器像素通常以“马赛克”形式排列，其中绿色像素的数量是红色和蓝色像素的两倍，这是因为人类视觉系统对绿色更为敏感。像素马赛克的一个影响是必须使用“去马赛克化”算法将这些传感器像素转换为图像像素，使红、绿、蓝色彩成分共置。 简单地直接使用马赛克像素块的颜色值的方法效果不佳，因为组成的传感器像素位置略有不同。
]

#parec[
  There are many challenges in designing digital sensors, most of them stemming from the small size of pixels, which is a result of demand for high-resolution images. The smaller a pixel is, the fewer photons it is exposed to given a lens and exposure time, and in turn, the harder it is to accurately measure the light. Pixel arrays suffer from a variety of types of noise, of which #emph[shot noise] is generally the most significant. It is due to the discrete nature of photons: there is random fluctuation in the number of photons that are counted, which matters more the fewer of them that are captured. Shot noise can be modeled using a Poisson distribution.
][
  设计数字传感器面临许多挑战，其中大多数源于像素的小尺寸，这是对高分辨率图像需求的结果。像素越小，在给定镜头和曝光时间下接收到的光子越少，从而更难准确测量光线。 像素阵列会受到多种类型噪声的影响，其中“光子噪声”通常是最显著的。这是由于光子的离散特性：所计数的光子数量存在随机波动，捕获的光子越少，这种波动的影响就越大。光子噪声可以用泊松分布建模。
]

#parec[
  Each pixel must receive a sufficient amount of light either to cause the necessary chemical reactions or to count enough photons to capture an accurate image. In Equation $(5.5)$, we saw that the energy captured at a pixel depends on the incident radiance, the pixel area, the exit pupil area, and the exposure time. With pixel area fixed for a given camera design, both increasing the lens aperture area and increasing the exposure time may introduce undesired side-effects in return for the additional light provided. A larger aperture reduces depth of field, which may lead to undesired defocus blur. Longer exposures can also cause blur due to moving objects in the scene or due to camera motion while the shutter is open. Sensors and film therefore provide an additional control in the form of an #emph[ISO
setting];.
][
  每个像素必须接收到足够的光线以引发必要的化学反应或计数足够的光子以捕获准确的图像。在方程（5.5）中，我们看到像素捕获的能量取决于入射辐射亮度、像素面积、出瞳面积和曝光时间。 在给定相机设计中，像素面积固定，增加镜头光圈面积和增加曝光时间可能会引入不期望的副作用，以换取额外的光线。较大的光圈会减少景深，可能导致不期望的失焦模糊。 较长的曝光时间也可能由于场景中移动的物体或快门打开时相机的运动而导致模糊。因此，传感器和胶卷提供了一种额外的控制形式，即ISO设置。
]

#parec[
  For physical film, ISO encodes its responsiveness to light (higher ISO values require less light to record an image). In digital cameras, ISO controls the #emph[gain];—a scaling factor that is applied to pixel values as they are read from the sensor. With physical cameras, increasing gain exacerbates noise, as noise in the initial pixel measurements is amplified. Because `pbrt` does not model the noise present in readings from physical sensors, the ISO value can be set arbitrarily to achieve a desired exposure.
][
  对于传统胶卷，ISO表示其对光的敏感度（较高的ISO值需要较少的光来记录图像）。在数码相机中，ISO控制“增益”——一个在从传感器读取像素值时应用的缩放因子。 对于物理相机，增加增益会加剧噪声，因为初始像素测量中的噪声会被放大。由于`pbrt`不对物理传感器读数中的噪声建模，因此ISO值可以任意设置以实现所需的曝光。
]

#parec[
  In `pbrt`'s sensor model, we model neither mosaicing nor noise, nor other effects like blooming, where a pixel that is exposed to enough light will "spill over" and start increasing the measured value at adjacent pixels. We also do not simulate the process of image readout from the sensor: many cameras use a #emph[rolling shutter] where scanlines are read in succession. For scenes with rapidly moving objects, this can give surprising results. Exercises at the end of the chapter suggest modifying `pbrt` in various ways to explore these effects.
][
  在`pbrt`的传感器模型中，我们既不对马赛克化也不对噪声进行建模，也不对其他效果进行建模，例如“溢出”，即一个像素接收到足够的光线会“溢出”并开始增加相邻像素的测量值。 我们也不模拟从传感器读取图像的过程：许多相机使用“滚动快门”，其中扫描线依次读取。对于快速移动的物体场景，这可能会产生令人惊讶的结果。 本章末尾的练习建议以各种方式修改`pbrt`以探索这些效果。
]

#parec[
  The `PixelSensor` class implements `pbrt`'s semi-idealized model of pixel color measurement. It is defined in the files `film.h` and `film.cpp`. #footnote[Its original implementation
is due to Anders Langlands and Luca Fascione and is based on the sensor
model in Weta Digital's _PhysLight_ system, which is used in Weta's
 _Manuka_ renderer.]
][
  `PixelSensor`类实现了`pbrt`的半理想化像素颜色测量模型。它在文件`film.h`和`film.cpp`中定义。#footnote[Its original implementation
is due to Anders Langlands and Luca Fascione and is based on the sensor
model in Weta Digital's _PhysLight_ system, which is used in Weta's
 _Manuka_ renderer.]
]

```cpp
// <<PixelSensor Definition>>=
class PixelSensor {
  public:
    // <<PixelSensor Public Methods>>
    static PixelSensor *Create(const ParameterDictionary &parameters, const RGBColorSpace *colorSpace,
                              Float exposureTime, const FileLoc *loc, Allocator alloc);
    static PixelSensor *CreateDefault(Allocator alloc = {});
    PixelSensor(Spectrum r, Spectrum g, Spectrum b,
          const RGBColorSpace *outputColorSpace, Spectrum sensorIllum,
          Float imagingRatio, Allocator alloc)
        : r_bar(r, alloc), g_bar(g, alloc), b_bar(b, alloc),
          imagingRatio(imagingRatio) {
        <<Compute XYZ from camera RGB matrix>>
    }
    PixelSensor(const RGBColorSpace *outputColorSpace, Spectrum sensorIllum,
          Float imagingRatio, Allocator alloc)
        : r_bar(&Spectra::X(), alloc), g_bar(&Spectra::Y(), alloc),
          b_bar(&Spectra::Z(), alloc), imagingRatio(imagingRatio) {
        <<Compute white balancing matrix for XYZ PixelSensor>>
    }
    RGB ToSensorRGB(SampledSpectrum L,
                    const SampledWavelengths &lambda) const {
        L = SafeDiv(L, lambda.PDF());
        return imagingRatio *
            RGB((r_bar.Sample(lambda) * L).Average(),
                (g_bar.Sample(lambda) * L).Average(),
                (b_bar.Sample(lambda) * L).Average());
    }

    // <<PixelSensor Public Members>>
    SquareMatrix<3> XYZFromSensorRGB;

  private:
    // <<PixelSensor Private Methods>>
    template <typename Triplet>
    static Triplet ProjectReflectance(Spectrum r, Spectrum illum,
                                      Spectrum b1, Spectrum b2, Spectrum b3);

    // <<PixelSensor Private Members>>
    DenselySampledSpectrum r_bar, g_bar, b_bar;
    Float imagingRatio;
    static constexpr int nSwatchReflectances = 24;
    static Spectrum swatchReflectances[nSwatchReflectances];

};
```

#parec[
  `PixelSensor` models three components of sensor pixels' operation:
][
  `PixelSensor`模型化传感器像素操作的三个组成部分：
]

#parec[
  + #emph[Exposure controls:] These are the user-settable parameters that
    control how bright or dark the image is.
][
  + #emph[曝光控制：] 这些是用户可设置的参数，用于控制图像的亮度或暗度。
]

#parec[
  #block[
    #set enum(numbering: "1.", start: 2)
    + #emph[RGB response:] `PixelSensor` uses spectral response curves that
      are based on measurements of physical camera sensors to model the
      conversion of spectral radiance to tristimulus colors.
  ]
][
  #block[
    #set enum(numbering: "1.", start: 2)
    + #emph[RGB响应：]
      `PixelSensor`使用基于物理相机传感器测量的光谱响应曲线来模拟光谱辐射亮度到三刺激颜色的转换。
  ]
]

#parec[
  #block[
    #set enum(numbering: "1.", start: 3)
    + #emph[White balance:] Cameras generally process the images they capture, including adjusting initial RGB values according to the color of illumination to model chromatic adaptation in the human visual system. Thus, captured images appear visually similar to what a human observer would remember having seen when taking a picture.
  ]
][
  #block[
    #set enum(numbering: "1.", start: 3)
    + #emph[白平衡：]
      相机通常会处理它们捕获的图像，包括根据照明颜色调整初始RGB值，以模拟人类视觉系统中的色彩适应。因此，捕获的图像在视觉上与人类观察者拍照时记忆中的景象相似。
  ]
]

#parec[
  `pbrt` includes a realistic camera model as well as idealized models based on projection matrices. Because pinhole cameras have apertures with infinitesimal area, we make some pragmatic trade-offs in the implementation of the `PixelSensor` so that images rendered with pinhole models are not completely black. We leave it the `Camera`'s responsibility to model the effect of the aperture size. The idealized models do not account for it at all, while the `RealisticCamera` does so in the `\<\<Compute weighting for RealisticCamera ray\>\>` fragment. The `PixelSensor` then only accounts for the shutter time and the ISO setting. These two factors are collected into a single quantity called the #emph[imaging ratio];.
][
  `pbrt`包括一个现实的相机模型以及基于投影矩阵的理想化模型。 由于针孔相机的光圈面积极小，我们在`PixelSensor`的实现中做出了一些实际的权衡，以便使用针孔模型渲染的图像不会完全是黑色的。 我们将光圈大小的效果建模的责任留给`Camera`。理想化模型完全不考虑这一点，而`RealisticCamera`在`\<\<Compute weighting for RealisticCamera ray\>\>`片段中考虑了这一点。 然后`PixelSensor`仅考虑快门时间和ISO设置。这两个因素被收集到一个称为“成像比”的单一数值中。
]

#parec[
  The `PixelSensor` constructor takes the sensor's RGB matching functions— $overline(r)$, $overline(g)$, and $overline(b)$ —and the imaging ratio as parameters. It also takes the color space requested by the user for the final output RGB values as well as the spectrum of an illuminant that specifies what color to consider to be white in the scene; together, these will make it possible to convert spectral energy to RGB as measured by the sensor and then to RGB in the output color space.
][
  `PixelSensor`构造函数接受传感器的RGB匹配函数—— $overline(r)$ 、 $overline(g)$ 和 $overline(b)$ ——以及成像比作为参数。 它还接受用户设定的最终输出的RGB的颜色空间以及在场景中被视为白色的光源光谱；这些将使得可以将传感器测量的光谱能量转换为RGB，然后转换为输出颜色空间中的RGB。
]

#parec[
  Figure 5.19 shows the effect of modeling camera response, comparing rendering a scene using the XYZ matching functions to compute initial pixel colors with rendering with the matching functions for an actual camera sensor.
][
  图5.19展示了建模相机响应的效果，比较了使用XYZ匹配函数计算初始像素颜色渲染场景与使用实际相机传感器匹配函数渲染场景的效果。
]

#figure(
  table(
    columns: 2,
    stroke: none,
    [#image("../pbr-book-website/4ed/Cameras_and_Film/zero-day-sensor-cie1931.png")],
    [#image("../pbr-book-website/4ed/Cameras_and_Film/zero-day-sensor-canon_eos_5d.png")],

    [(a) CIE 1931], [(b) Canon EOS 5D],
  ),
  caption: [
    #ez_caption[Figure 5.19: The Effect of Accurately Modeling Camera Sensor Response. (a) Scene rendered using the XYZ matching functions for the PixelSensor. (b) Scene rendered using measured sensor response curves for a Canon EOS 5D camera. Note that the color tones are slightly cooler—they have less orange and more blue to them. (Scene courtesy of Beeple.)][准确模拟相机传感器响应的效果。(a) 使用 PixelSensor 的 XYZ 匹配函数渲染的场景。(b) 使用佳能 EOS 5D 相机的实测传感器响应曲线渲染的场景。注意，颜色调性略显冷色调——相比之下，橙色较少，蓝色较多。（场景由 Beeple 提供。）]
  ],
  kind: image,
)

```cpp
// <<PixelSensor Public Methods>>=
PixelSensor(Spectrum r, Spectrum g, Spectrum b,
       const RGBColorSpace *outputColorSpace, Spectrum sensorIllum,
       Float imagingRatio, Allocator alloc)
    : r_bar(r, alloc), g_bar(g, alloc), b_bar(b, alloc),
      imagingRatio(imagingRatio) {
    // <<Compute XYZ from camera RGB matrix>>
}
```
```cpp
//<<PixelSensor Private Members>>=
DenselySampledSpectrum r_bar, g_bar, b_bar;
Float imagingRatio;
```

#parec[
  The RGB color space in which a sensor pixel records light is generally not the same as the RGB color space that the user has specified for the final image. The former is generally specific to a camera and is determined by the physical properties of its pixel color filters, and the latter is generally a device-independent color space like sRGB or one of the other color spaces described in Section 4.6.3. Therefore, the `PixelSensor` constructor computes a $3 times 3$ matrix that converts from its RGB space to XYZ. From there, it is easy to convert to a particular output color space.
][
  传感器像素记录光的RGB颜色空间通常与用户为最终图像指定的RGB颜色空间不同。 前者通常特定于某个相机，由其像素颜色滤光片的物理特性决定，而后者通常是设备无关的颜色空间，如sRGB或第4.6.3节中描述的其他颜色空间。 因此，`PixelSensor`构造函数计算一个从其RGB空间到XYZ的 $3 times 3$ 矩阵。 从那里，很容易转换到特定的输出颜色空间。
]


#parec[
  This matrix is found by solving an optimization problem. It starts with over twenty spectral distributions, representing the reflectance of patches with a variety of colors from a standardized color chart. The constructor computes the RGB colors of those patches under the camera's illuminant in the camera's color space as well as their XYZ colors under the illuminant of the output color space. If these colors are respectively denoted by column vectors, then we can consider the problem of finding a $3 times 3$ matrix $upright(bold(M))$.
][
  这个矩阵是通过求解一个优化问题找到的。它从二十多个光谱分布开始，代表了来自标准化颜色图表的各种颜色的反射率。 构造函数计算这些色块在相机光源下的RGB颜色以及它们在输出颜色空间光源下的XYZ颜色。 如果这些颜色分别用列向量表示，那么我们可以考虑找到一个 $3 times 3$ 矩阵 $upright(bold(M))$ 的问题。
]


$
  upright(bold(M)) mat(delim: "[", r_1, r_2, , r_n; g_1, g_2, dots.h.c, g_n; b_1, b_2, , b_n) approx mat(delim: "[", x_1, x_2, , x_n; y_1, y_2, dots.h.c, y_n; z_1, z_2, , z_n)
$


#parec[
  As long as there are more than three reflectances, this is an over-constrained problem that can be solved using linear least squares.
][
  只要反射率超过三个，这就构成了一个超约束问题，可以使用线性最小二乘法解决。
]

```cpp
// <<Compute XYZ from camera RGB matrix>>=
// <<Compute rgbCamera values for training swatches>>
Float rgbCamera[nSwatchReflectances][3];
for (int i = 0; i < nSwatchReflectances; ++i) {
    RGB rgb = ProjectReflectance<RGB>(swatchReflectances[i], sensorIllum,
                                      &r_bar, &g_bar, &b_bar);
    for (int c = 0; c < 3; ++c)
        rgbCamera[i][c] = rgb[c];
}

// <<Compute xyzOutput values for training swatches>>
Float xyzOutput[24][3];
Float sensorWhiteG = InnerProduct(sensorIllum, &g_bar);
Float sensorWhiteY = InnerProduct(sensorIllum, &Spectra::Y());
for (size_t i = 0; i < nSwatchReflectances; ++i) {
    Spectrum s = swatchReflectances[i];
    XYZ xyz = ProjectReflectance<XYZ>(s, &outputColorSpace->illuminant, &Spectra::X(),
                                      &Spectra::Y(), &Spectra::Z()) *
              (sensorWhiteY / sensorWhiteG);
    for (int c = 0; c < 3; ++c)
        xyzOutput[i][c] = xyz[c];
}

// <<Initialize XYZFromSensorRGB using linear least squares>>
pstd::optional<SquareMatrix<3>> m =
    LinearLeastSquares(rgbCamera, xyzOutput, nSwatchReflectances);
if (!m) ErrorExit("Sensor XYZ from RGB matrix could not be solved.");
XYZFromSensorRGB = *m;
```

#parec[
  Given the sensor's illuminant, the work of computing the RGB coefficients for each reflectance is handled by the `ProjectReflectance()` method.
][
  鉴于传感器的光源，计算每个反射率的RGB系数的工作由 `ProjectReflectance()` 方法处理。
]

```cpp
// <<Compute rgbCamera values for training swatches>>=
Float rgbCamera[nSwatchReflectances][3];
for (int i = 0; i < nSwatchReflectances; ++i) {
    RGB rgb = ProjectReflectance<RGB>(swatchReflectances[i], sensorIllum,
                                      &r_bar, &g_bar, &b_bar);
    for (int c = 0; c < 3; ++c)
        rgbCamera[i][c] = rgb[c];
}
```

#parec[
  For good results, the spectra used for this optimization problem should present a good variety of representative real-world spectra. The ones used in `pbrt` are based on measurements of a standard color chart.#footnote[These reflectance measurements are courtesy of Danny
Pascale and are used with permission.]
][
  为了取得好的结果，用于此优化问题的光谱应呈现出一系列具有代表性的真实世界光谱。`pbrt` 中使用的光谱基于标准色彩图表的测量。
]
```cpp
// <<PixelSensor Private Members>>+=
static constexpr int nSwatchReflectances = 24;
static Spectrum swatchReflectances[nSwatchReflectances];
```
#parec[
  The `ProjectReflectance()` utility method takes spectral distributions for a reflectance and an illuminant as well as three spectral matching functions $overline(b)_i$ for a tristimulus color space. It returns a triplet of color coefficients $c_i$.
][
  `ProjectReflectance()` 工具方法接受反射率和光源的光谱分布以及三种用于三刺激颜色空间的光谱匹配函数 $overline(b)_i$。它返回一组三色系数 $c_i$。
]

$
  c_i = integral r(lambda) L(lambda) overline{b}_i(lambda) \, d lambda,
$

#parec[
  where $r$ is the spectral reflectance function, $L$ is the illuminant's spectral distribution, and $overline(b)_i$ is a spectral matching function.Under the assumption that the second matching function $overline(b)_2$ generally corresponds to luminance or at least something green, the color that causes the greatest response by the human visual system, the returned color triplet is normalized by $integral L(lambda)overline(b)_2(lambda) d lambda$. In this way, the linear least squares fit at least roughly weights each RGB/XYZ pair according to visual importance.
][
  其中， $r$ 是光谱反射率函数， $L$ 是光源的光谱分布， $overline(b)_i$ 是光谱匹配函数。在假设第二个匹配函数 $overline(b)_2$ 通常对应亮度，或者至少对应绿色——即引起人类视觉系统最大反应的颜色的前提下，返回的颜色三元组通过 $integral L(lambda)overline(b)_2(lambda) d lambda$ 进行归一化。通过这种方式，线性最小二乘拟合至少能够大致根据视觉重要性对每个 RGB/XYZ 对进行加权。
]

#parec[
  The `ProjectReflectance()` utility function takes the color space triplet type as a template parameter and is therefore able to return both RGB and XYZ values as appropriate.Its implementation follows the same general form as Spectrum::InnerProduct(), computing a Riemann sum over 1 nm spaced wavelengths, so it is not included here.
][
  `ProjectReflectance()` 工具函数将颜色空间三元组类型作为模板参数，因此能够返回RGB和XYZ值。它的实现遵循与 Spectrum::InnerProduct() 相同的一般形式，计算 1 纳米间隔波长上的黎曼和，因此这里不再列出。
]
```cpp
template <typename Triplet>
static Triplet ProjectReflectance(Spectrum r, Spectrum illum,
                                  Spectrum b1, Spectrum b2, Spectrum b3);
```
#parec[
  The fragment that computes XYZ coefficients in the output color space, `<<Compute xyzOutput values for training swatches>>`, is generally similar to the one for RGB, with the differences that it uses the output illuminant and the XYZ spectral matching functions and initializes the xyzOutput array. It is therefore also not included here.
][
  计算输出颜色空间中 XYZ 系数的片段 `<<计算用于训练色块的 xyzOutput 值>>`，通常与 RGB 的计算相似，区别在于它使用了输出光源和 XYZ 光谱匹配函数，并初始化了 xyzOutput 数组。因此，这里也不再列出。
]

#parec[
  Given the two matrices of color coefficients, a call to the `LinearLeastSquares()` function solves the optimization problem of Equation (5.7).
][
  给定两个颜色系数矩阵，通过调用 `LinearLeastSquares()` 函数可以求解方程 (5.7) 的优化问题。
]
```cpp
pstd::optional<SquareMatrix<3>> m =
    LinearLeastSquares(rgbCamera, xyzOutput, nSwatchReflectances);
if (!m) ErrorExit("Sensor XYZ from RGB matrix could not be solved.");
XYZFromSensorRGB = *m;
```

#parec[
  Because the RGB and XYZ colors are computed using the color spaces' respective illuminants, the matrix $upright(bold(M))$ also performs white balancing.
][
  由于RGB和XYZ颜色是使用各自颜色空间的光源计算的，因此矩阵 $upright(bold(M))$ 还执行白平衡。
]
```cpp
SquareMatrix<3> XYZFromSensorRGB;
```
#parec[
  A second `PixelSensor` constructor uses XYZ matching functions for the pixel sensor's spectral response curves. If a specific camera sensor is not specified in the scene description file, this is the default.Note that with this usage, the member variables r_bar, g_bar, and b_bar are misnamed in that they are actually $X$, $Y$, and $Z$.
][
  第二个 PixelSensor 构造函数使用 XYZ 匹配函数来表示像素传感器的光谱响应曲线。如果场景描述文件中未指定特定的摄像头传感器，这将是默认设置。需要注意的是，在这种用法下，成员变量 r_bar、g_bar 和 b_bar 命名不当，因为它们实际上分别代表 $X$ 、 $Y$ 和 $Z$。
]

```cpp
PixelSensor(const RGBColorSpace *outputColorSpace, Spectrum sensorIllum,
       Float imagingRatio, Allocator alloc)
    : r_bar(&Spectra::X(), alloc), g_bar(&Spectra::Y(), alloc),
      b_bar(&Spectra::Z(), alloc), imagingRatio(imagingRatio) {
    <<Compute white balancing matrix for XYZ PixelSensor>>
    if (sensorIllum) {
        Point2f sourceWhite = SpectrumToXYZ(sensorIllum).xy();
        Point2f targetWhite = outputColorSpace->w;
        XYZFromSensorRGB = WhiteBalance(sourceWhite, targetWhite);
    }
}
```

#parec[
  By default, no white balancing is performed when `PixelSensor` converts to XYZ coefficients; that task is left for post-processing. However, if the user does specify a color temperature, white balancing is handled by the `XYZFromSensorRGB` matrix. (It is otherwise the identity matrix.) The `WhiteBalance()` function that computes this matrix will be described shortly; it takes the chromaticities of the white points of two color spaces and returns a matrix that maps the first to the second.
][
  默认情况下，当 `PixelSensor` 转换为 XYZ 系数时，不执行白平衡；这一任务留待后期处理。然而，如果用户指定了色温，则白平衡由 `XYZFromSensorRGB` 矩阵处理。（否则，它将是单位矩阵。）稍后将描述计算该矩阵的 `WhiteBalance()` 函数；它接受两个颜色空间的白点色度，并返回一个将第一个映射到第二个的矩阵。
]

```cpp
if (sensorIllum) {
     Point2f sourceWhite = SpectrumToXYZ(sensorIllum).xy();
     Point2f targetWhite = outputColorSpace->w;
     XYZFromSensorRGB = WhiteBalance(sourceWhite, targetWhite);
}
```
#parec[
  The main functionality provided by the `PixelSensor` is the `ToSensorRGB()` method, which converts a point-sampled spectral distribution $L(lambda_i)$ to RGB coefficients in the sensor's color space.It does so via Monte Carlo evaluation of the sensor response integral, Equation (5.6), giving estimators of the form
][
  `PixelSensor` 提供的主要功能是 `ToSensorRGB()` 方法，该方法将点采样的光谱分布 $L(lambda_i)$ 转换为传感器颜色空间中的 RGB 系数。它通过对传感器响应积分（方程 (5.6)）进行蒙特卡罗评估，以以下形式给出估计器。
]
$ r approx 1 / n sum_(i = 1)^n frac(L (lambda_i) r^(‾) (lambda_i), p (lambda_i)) , $

#parec[
  where $n$ is equal to NSpectrumSamples. The associated PDF values are available from the SampledWavelengths and the sum over wavelengths and division by $n$ is handled using SampledSpectrum::Average(). These coefficients are scaled by the imaging ratio, which completes the conversion.
][
  其中 $n$ 等于 NSpectrumSamples。相关的概率密度函数（PDF）值可以从 SampledWavelengths 中获得，波长的求和和除以 $n$ 的操作通过 SampledSpectrum::Average() 处理。这些系数通过成像比例进行缩放，以完成转换。
]

```cpp
RGB ToSensorRGB(SampledSpectrum L,
                const SampledWavelengths &lambda) const {
    L = SafeDiv(L, lambda.PDF());
    return imagingRatio *
        RGB((\bar{r}.Sample(lambda) * L).Average(),
            (g_bar.Sample(lambda) * L).Average(),
            (b_bar.Sample(lambda) * L).Average());
}
```

==== Chromatic Adaptation and White Balance

#parec[
  One of the remarkable properties of the human visual system is that the color of objects is generally seen as the same, even under different lighting conditions; this effect is called chromatic adaptation. Cameras perform a similar function so that photographs capture the colors that the person taking the picture remembers seeing; in that context, this process is called white balancing.
][
  人类视觉系统的一个显著特性是，即使在不同的光照条件下，物体的颜色通常也被看作是相同的；这种效果被称为色度适应。相机执行类似的功能，以便照片捕捉到拍摄者记忆中看到的颜色；在这种情况下，这个过程被称为白平衡。
]

#parec[
  `pbrt` provides a `WhiteBalance()` function that implements a white balancing algorithm called the von Kries transform. It takes two chromaticities: one is the chromaticity of the illumination and the other the chromaticity of the color white. (Recall the discussion in Section~4.6.3 of why white is not usually a constant spectrum but is instead defined as the color that humans perceive as white.) It returns a $3 times 3$ matrix that applies the corresponding white balancing operation to XYZ colors.
][
  `pbrt` 提供了一个 `WhiteBalance()` 函数，该函数实现了一种称为冯·克里斯变换的白平衡算法。它需要两个色度：一个是照明的色度，另一个是白色的色度。（回想一下在第~4.6.3节中讨论的，为什么白色通常不是一个恒定的光谱，而是定义为人类感知为白色的颜色。）它返回一个用于 XYZ 颜色白平衡的 $3 times 3$ 矩阵。
]

```cpp
<<White Balance Definitions>>=
SquareMatrix<3> WhiteBalance(Point2f srcWhite, Point2f targetWhite) {
    <<Find LMS coefficients for source and target white>>
       XYZ srcXYZ = XYZ::FromxyY(srcWhite), dstXYZ = XYZ::FromxyY(targetWhite);
       auto srcLMS = LMSFromXYZ * srcXYZ, dstLMS = LMSFromXYZ * dstXYZ;

    <<Return white balancing matrix for source and target white>>
       SquareMatrix<3> LMScorrect = SquareMatrix<3>::Diag(
           dstLMS[0] / srcLMS[0], dstLMS[1] / srcLMS[1], dstLMS[2] / srcLMS[2]);
       return XYZFromLMS * LMScorrect * LMSFromXYZ;

}
```

#parec[
  White balance with the von Kries transform is performed in the _LMS color
  space_, which is a color space where the responsivity of the three matching functions is specified to match the three types of cone in the human eye. By performing white balancing in the LMS space, we can model the effect of modulating the contribution of each type of cone in the eye, which is believed to be how chromatic adaptation is implemented in humans. After computing normalized XYZ colors corresponding to the provided chromaticities, the LMSFromXYZ matrix can be used to transform to LMS from XYZ.
][
  使用冯·克里斯变换进行白平衡是在_LMS颜色空间_中进行的，这是一种颜色空间，其中三个匹配函数的响应性被指定为匹配人眼中的三种锥体类型。通过在 LMS 空间中进行白平衡，我们可以模拟调节眼中每种锥体贡献的效果，这被认为是人类实现色度适应的方式。在计算出与提供的色度相对应的归一化 XYZ 颜色后，可以使用 LMSFromXYZ 矩阵将 XYZ 转换为 LMS。
]

```cpp
XYZ srcXYZ = XYZ::FromxyY(srcWhite), dstXYZ = XYZ::FromxyY(targetWhite);
auto srcLMS = LMSFromXYZ * srcXYZ, dstLMS = LMSFromXYZ * dstXYZ;
```


#parec[
  $3 times 3$ matrices that convert between LMS and XYZ are available as constants.
][
  用于在 LMS 和 XYZ 之间转换的 $3 times 3$ 矩阵可作为常量使用。
]

#parec[
  Given a color in LMS space, white balancing is performed by dividing out the color of the scene's illuminant and then multiplying by the color of the desired illuminant, which can be represented by a diagonal matrix. The complete white balance matrix that operates on XYZ colors follows directly.
][
  在 LMS 空间中进行白平衡时，首先去除场景光源的颜色，然后乘以目标光源的颜色，这可以用对角矩阵表示。直接得到的完整白平衡矩阵作用于 XYZ 颜色。
]

```cpp
SquareMatrix<3> LMScorrect = SquareMatrix<3>::Diag(
  dstLMS[0] / srcLMS[0], dstLMS[1] / srcLMS[1], dstLMS[2] / srcLMS[2]);
return XYZFromLMS * LMScorrect * LMSFromXYZ; }
```


#parec[
  Figure~5.20 shows an image rendered with a yellowish illuminant and the image after white balancing with the illuminant's chromaticity.
][
  图~5.20显示了一幅用黄色光源渲染的图像以及用光源色度进行白平衡后的图像。
]

#figure(
  table(
    columns: 2,
    stroke: none,
    [#image("../pbr-book-website/4ed/Cameras_and_Film/staircase-orig.png")],
    [#image("../pbr-book-website/4ed/Cameras_and_Film/staircase-wb.png")],

    [(a) Scene With Yellow Illuminant], [(b) Whitebalanced Image],
  ),
  caption: [
    #ez_caption[
      Figure 5.20: The Effect of White Balance. (a)~Image of a scene with a
      yellow illuminant that has a similar spectral distribution to an
      incandescent light bulb. (b)~White balanced image, using a color
      temperature of 3000~K. Due to chromatic adaptation, this image is much
      closer than (a) to what a human observer would perceive viewing this
      scene. (Scene courtesy of Wig42 from Blend Swap, via Benedikt Bitterli.)

    ][
      图 5.20:
      白平衡的效果。(a)~场景的图像，使用与白炽灯泡类似光谱分布的黄色光源。(b)~白平衡后的图像，使用
      3000~K 的色温。由于色度适应，这幅图像比 (a)
      更接近人类观察者在观看此场景时的感知。(场景由 Wig42 提供，来自 Blend
      Swap，通过 Benedikt Bitterli。)
    ]
  ],
  kind: image,
)

==== Sampling Sensor Response

#parec[
  Because the sensor response functions used by a PixelSensor describe the sensor's wavelength-dependent response to radiance, it is worth at least approximately accounting for their variation when sampling the wavelengths of light that a ray is to carry. At minimum, a wavelength where all of them are zero should never be chosen, as that wavelength will make no contribution to the final image. More generally, applying importance sampling according to the sensor response functions is desirable as it offers the possibility of reducing error in the estimates of Equation~(5.8).
][
  由于 PixelSensor 的传感器响应函数描述了传感器对辐射的波长依赖性，因此在采样光波长时应考虑其变化。至少，永远不应选择所有函数值都为零的波长，因为该波长将对最终图像没有贡献。更一般地，根据传感器响应函数应用重要性采样是可取的，因为它提供了减少方程~(5.8) 估计误差的可能性。
]

#parec[
  However, choosing a distribution to use for sampling is challenging since the goal is minimizing error perceived by humans rather than strictly minimizing numeric error. Figure~5.21(a) shows the plots of both the CIE $Y$ matching function and the sum of $X$, $Y$, and $Z$ matching functions, both of which could be used. In practice, sampling according to $Y$ alone gives excessive chromatic noise, but sampling by the sum of all three matching functions devotes too many samples to wavelengths between 400~nm and 500~nm, which are relatively unimportant visually.
][
  然而，选择用于采样的分布是具有挑战性的，因为目标是最小化人类感知的误差，而不是严格最小化数值误差。图~5.21(a) 显示了 CIE $Y$ 匹配函数和 $X$ 、 $Y$ 和 $Z$ 匹配函数之和的图，两者都可以使用。在实践中，仅根据 $Y$ 采样会产生过多的色度噪声，但根据所有三个匹配函数之和采样会将过多的样本分配给 400~nm 到 500~nm 之间的波长，这在视觉上相对不重要。
]

#parec[
  A parametric probability distribution function that balances these concerns and works well for sampling the visible wavelengths is
][
  一种平衡这些问题并在采样可见波长时效果良好的参数化概率分布函数是
]


$
  p_v (lambda) = (integral_(lambda_(upright("min")))^(lambda_(upright("max"))) f (lambda) thin d lambda)^(- 1) f ( lambda ) ,
$

#parec[
  with
][
  其中，函数定义为
]

$ f (lambda) = frac(1, cosh^2 (A (lambda - B))) , $


#parec[
  $A = 0.0072 thin upright("n m")^(- 1)$, and $B = 538 thin upright("n m")$. Figure~5.21(b) shows a plot of $p_v (lambda)$.
][
  $A = 0.0072 thin upright("n m")^(- 1)$， $B = 538 thin upright("n m")$。图~5.21(b)展示了 $p_v (lambda)$ 的曲线图。
]


#figure(
  image("../pbr-book-website/4ed/Cameras_and_Film/pha05f21.svg"),
  caption: [
    #ez_caption[ (a) Plot of normalized PDFs corresponding to the CIE $Y$ matching function and the sum of the $X$, $Y$, and $Z$ matching functions. (b) Plot of the parametric distribution $p_v(lambda)$ from Equation (5.9).][]
  ],
)
#parec[
  Our implementation samples over the wavelength range from $360 thin upright("n m")$ to $830 thin upright("n m")$. The normalization constant that converts $f$ into a PDF is precomputed.
][
  我们的实现对波长范围从 $360 thin upright("n m")$ 到 $830 thin upright("n m")$ 进行采样。将 $f$ 转换为PDF的归一化常数已预先计算。
]


```cpp
Float VisibleWavelengthsPDF(Float lambda) {
    if (lambda < 360 || lambda > 830)
        return 0;
    return 0.0039398042f / Sqr(std::cosh(0.0072f * (lambda - 538)));
}
```

#parec[
  The PDF can be sampled using the inversion method; the result is implemented in SampleVisibleWavelengths().
][
  可以使用反演法对PDF进行采样；其结果在函数SampleVisibleWavelengths()中实现。
]


```cpp
Float SampleVisibleWavelengths(Float u) {
    return 538 - 138.888889f * std::atanh(0.85691062f - 1.82750197f * u);
}
```

#parec[
  We can now implement another sampling method in the SampledWavelengths class, SampleVisible(), which uses this technique.
][
  我们现在可以在SampledWavelengths类中实现另一种采样方法SampleVisible()，使用这种技术。
]

```cpp
static SampledWavelengths SampleVisible(Float u) {
    SampledWavelengths swl;
    for (int i = 0; i < NSpectrumSamples; ++i) {
        &lt;&lt;Compute up for ith wavelength sample&gt;&gt;           Float up = u + Float(i) / NSpectrumSamples;
        if (up > 1)
            up -= 1;
        swl.lambda[i] = SampleVisibleWavelengths(up);
        swl.pdf[i] = VisibleWavelengthsPDF(swl.lambda[i]);
    }
    return swl;
}
```

#parec[
  Like `SampledWavelengths::SampleUniform()`, `SampleVisible()` uses a single random sample to generate all wavelength samples. It uses a slightly different approach, taking uniform steps across the $\[ 0 , 1 \)$ sample space before sampling each wavelength.
][
  与SampledWavelengths::SampleUniform()类似，SampleVisible()使用单个随机样本生成所有波长样本。它使用稍微不同的方法，在 $\[ 0 , 1 \)$ \$样本空间中采取均匀步长，然后对每个波长进行采样。
]


```cpp
Float up = u + Float(i) / NSpectrumSamples;
if (up > 1)
    up -= 1;
```

#parec[
  Using this distribution for sampling in place of a uniform distribution is worthwhile. Figure~5.22 shows two images of a scene, one rendered using uniform wavelength samples and the other rendered using SampleVisible(). Color noise is greatly reduced, with only a 1% increase in runtime.
][
  在采样中使用这种分布代替均匀分布是有价值的。图~5.22显示了一个场景的两幅图像，一幅使用均匀波长样本渲染，另一幅使用SampleVisible()渲染。 色彩噪声明显减少，而运行时间仅增加1%。
]


#figure(
  table(
    columns: 2,
    stroke: none,
    [#image("../pbr-book-website/4ed/Cameras_and_Film/lte-orb-wavelength-uniform.png")],
    [#image("../pbr-book-website/4ed/Cameras_and_Film/lte-orb-wavelength-visible.png")],

    [(a) Uniform sampling], [(b) Importance sampling],
  ),
  caption: [
    #ez_caption[
      Figure 5.22: (a)~Scene rendered with 4~samples per pixel, each with
      4~wavelength samples, sampled uniformly over the visible range.
      (b)~Rendered at the same sampling rates but instead sampling wavelengths
      using SampledWavelengths::SampleVisible(). This image has substantially
      less color noise, at a negligible cost in additional computation. (Model
      courtesy of Yasutoshi Mori.)
    ][
      图 5.22:
      (a)~场景以每像素4个样本渲染，每个样本有4个波长样本，在可见范围内均匀采样。
      (b)~以相同的采样率渲染，但使用SampledWavelengths::SampleVisible()进行波长采样。此图像的色彩噪声明显减少，计算成本几乎可以忽略不计。
      (模型由Yasutoshi Mori提供。)
    ]
  ],
  kind: image,
)
#parec

=== Filtering Image Samples
<filtering-image-samples>


#parec[
  The main responsibility of `Film` implementations is to aggregate multiple spectral samples at each pixel in order to compute a final value for it. In a physical camera, each pixel integrates light over a small area. Its response may have some spatial variation over that area that depends on the physical design of the sensor. In Chapter8 we will consider this operation from the perspective of signal processing and will see that the details of where the image function is sampled and how those samples are weighted can significantly affect the final image quality.
][
  `Film`实现的主要责任是聚合每个像素的多个光谱样本以计算其最终值。在物理相机中，每个像素在一个小区域上累积光。其响应可能在该区域上有一些空间变化，这取决于传感器的物理设计。在第8章中，我们将从信号处理的角度考虑这一操作，并将看到图像函数采样位置和样本加权方式的细节可以显著影响最终图像质量。
]

#parec[
  Pending those details, for now we will assume that some filter function $f$ is used to define the spatial variation in sensor response around each image pixel. These filter functions quickly go to zero, encoding the fact that pixels only respond to light close to them on the film. They also encode any further spatial variation in the pixel's response. With this approach, if we have an image function $r(x, y)$ that gives the red color at an arbitrary position on the film (e.g., as measured using a sensor response function $overline(r)(lambda)$ with Equation 5.6 ), then the filtered red value $r_f$ at a position $(x, y)$ is given by
][
  在这些细节尚未明确之前，我们暂时假设使用某种滤波函数 $f$ 来定义传感器响应在每个图像像素周围的空间变化。这些滤波函数迅速趋于零，编码了像素仅对胶片上接近它们的光线做出响应的事实。它们还编码了像素响应的任何进一步空间变化。通过这种方法，如果我们有一个图像函数 $r(x, y)$，它给出了胶片上任意位置的红色（例如，使用传感器响应函数 $overline(r)(lambda)$ 测量，方程5.6，则在位置 $(x, y)$ 的过滤红色值 $r_f$ 由以下公式给出
]

$ r_f (x , y) = integral f (x - x prime , y - y prime) r (x prime , y prime) thin d x prime thin d y prime , $

#parec[
  where the filter function $f$ is assumed to integrate to $1$.
][
  其中假设滤波函数 $f$ 的积分为 $1$。
]

#parec[
  As usual, we will estimate this integral using point samples of the image function. The estimator is
][
  通常，我们使用图像函数的点样本来估计此积分。其估计器为
]

$ r_f (x , y) approx 1 / n sum_(i = 1)^n frac(f (x - x_i , y - y_i) r (x_i , y_i), p (x_i , y_i)) . $


#parec[
  Two approaches have been used in graphics to sample the integrand. The first, which was used in all three previous versions of `pbrt`, is to sample the image uniformly. Each image sample may then contribute to multiple pixels' final values, depending on the extent of the filter function being used. This approach gives the estimator
][
  在计算机图形学中有两种方法用于采样被积函数。第一种方法，在之前的三个版本的 `pbrt` 中使用过，是均匀采样图像。然后，每个图像样本可能会根据所用滤波函数的范围对多个像素的最终值产生贡献。此方法给出的估计器为
]

$ r_f (x , y) approx A / n sum_(i = 1)^n f (x - x_i , y - y_i) r (x_i , y_i) , $


#parec[
  where $A$ is the film area. Figure 5.23 illustrates the approach; it shows a pixel at location $(x , y)$ that has a pixel filter with extent `radius.x` in the $x$ direction and `radius.y` in the $y$ direction. All the samples at positions $(x_i , y_i)$ inside the box given by the filter extent may contribute to the pixel's value, depending on the filter function's value for $f \( x - x_i , y - y_i$ ).
][
  其中 $A$ 是胶片面积。图 5.23 说明了这种方法；它展示了一个位于 $(x , y)$ 的像素，该像素具有在 $x$ 方向上 `radius.x` 和在 $y$ 方向上 `radius.y` 范围的像素滤波器。所有位于由滤波范围给定的框内的 $(x_i , y_i)$ 位置的样本可能会对像素的值做出贡献，这取决于滤波函数对 $f (x - x_i , y - y_i)$ 的值。
]

#figure(
  image("../pbr-book-website/4ed/Cameras_and_Film/pha05f23.svg"),
  caption: [
    #ez_caption[
      2D Image Filtering. To compute a filtered pixel value for the pixel marked with a filled circle located at $(x,y)$, all the image samples inside the box around $(x,y)$ with extent radius.x and radius.y need to be considered. Each of the image samples $(x_i, y_i)$, denoted by open circles, is weighted by a 2D filter function, $f(x-x_i, y - y_i)$. The weighted average of all samples is the final pixel value.
    ][
      二维图像滤波。要计算标记为实心圆的像素 $(x, y)$ 的滤波像素值，需要考虑位于 $(x, y)$ 周围、范围为 radius.x 和 radius.y 的框内的所有图像样本。每个图像样本 $(x_i, y_i)$，用空心圆表示，都通过二维滤波函数 $f(x - x_i, y - y_i)$ 加权。所有样本的加权平均值就是最终的像素值。
    ]
  ],
)
#parec[
  While Equation (5.12) gives an unbiased estimate of the pixel value, variation in the filter function leads to variance in the estimates. Consider the case of a constant image function $r$ : in that case, we would expect the resulting image pixels to all be exactly equal to $r$. However, the sum of filter values $f (x - x_i , y - y_i)$ will not generally be equal to $1$ : it only equals $1$ in expectation. Thus, the image will include noise, even in this simple setting. If the alternative estimator
][
  虽然方程 (5.12) 给出了像素值的无偏估计，但滤波函数的变化会导致估计值的方差。考虑一个常量图像函数 $r$ 的情况：在这种情况下，我们期望生成的图像像素都完全等于 $r$。然而，滤波值 $f (x - x_i , y - y_i)$ 的和通常不会等于 $1$ ：它仅在期望上等于 $1$。因此，即使在这种简单的设置中，图像也会包含噪声。如果使用替代估计器
]

$ r_f (x , y) approx frac(sum_i f (x - x_i , y - y_i) r (x_i , y_i), sum_i f (x - x_i , y - y_i)) . $


#parec[
  is used instead, that variance is eliminated at the cost of a small amount of bias. (This is the #emph[weighted importance sampling] Monte Carlo estimator.) In practice, this trade-off is worthwhile.
][
  则可以消除这种方差，但代价是引入少量的偏差。（这是 #emph[加权重要性采样] 蒙特卡罗估计器。）在实践中，这种权衡是值得的。
]

#parec[
  Equation (5.10) can also be estimated independently at each pixel. This is the approach used in this version of `pbrt`. In this case, it is worthwhile to sample points on the film using a distribution based on the filter function. This approach is known as #emph[filter importance
sampling];. With it, the spatial variation of the filter is accounted for purely via the distribution of sample locations for a pixel rather than scaling each sample's contribution according to the filter's value.
][
  方程 (5.10) 也可以在每个像素处独立估计。这是此版本的 `pbrt` 中使用的方法。在这种情况下，值得在胶片上使用基于滤波函数分布的采样点。这种方法称为 #emph[滤波重要性采样];。通过它，滤波的空间变化完全通过像素的样本位置分布来考虑，而不是根据滤波的值来缩放每个样本的贡献。
]

#parec[
  If $p prop f$, then those two factors cancel in Equation (5.11) and we are left with an average of the $r (x_i , y_i)$ sample values scaled by the constant of proportionality. However, here we must handle the rare (for rendering) case of estimating an integral that may be negative: as we will see in Chapter 8, filter functions that are partially negative can give better results than those that are nonnegative. In that case, we have $p prop lr(|f|)$, which gives
][
  如果 $p prop f$，那么在方程 (5.11) 中这两个因子会相互抵消，我们剩下的是 $r (x_i , y_i)$ 样本值的平均值，按比例常数缩放。然而，在这里我们必须处理估计可能为负的积分的罕见（对于渲染来说）情况：正如我们将在第 8 章中看到的，部分为负的滤波函数可以比非负的函数提供更好的结果。在这种情况下，我们有 $p prop lr(|f|)$，这给出
]

$
  r_f (x , y) approx (integral lr(|f (x prime , y prime)|) thin d x prime thin d y prime) ( 1 / n sum_(i = 1)^n upright("sgn") (f (x - x_i , y - y_i)) r (x_i , y_i) ) ,
$

#parec[
  where $"sign"(x)$ is $1$ if $x > 0$, $0$ if it is $0$, and $- 1$ otherwise. However, this estimator has the same problem as Equation (5.12): even with a constant function $r$, the estimates will have variance depending on how many of the $"sign"$ function evaluations give $1$ and how many give $- 1$.
][
  其中 $"sign"(x)$ 为 $1$ 如果 $x > 0$，为 $0$ 如果 $x = 0$，否则为 $- 1$。然而，这个估计器与方程 (5.12) 存在相同的问题：即使对于常量函数 $r$，估计值的方差仍然取决于 $"sign"$ 函数评估中给出 $1$ 的数量和给出 $- 1$ 的数量。
]

#parec[
  Therefore, this version of `pbrt` continues to use the weighted importance sampling estimator, computing pixel values as
][
  因此，此版本的 `pbrt` 继续使用加权重要性采样估计器，计算像素值为
]

$
  r_(upright(f)) (x, y) approx frac(sum_i w(x - x_i comma y - y_i) r(x_i comma y_i), sum_i w(x - x_i comma y - y_i))
$
#parec[
  with $w (x , y) = f (x , y)\/ p (x , y)$.
][
  其中 $w (x , y) = f (x , y)\/ p (x , y)$。
]

#parec[
  The first of these two approaches has the advantage that each image sample can contribute to multiple pixels' final filtered values. This can be beneficial for rendering efficiency, as all the computation involved in computing the radiance for an image sample can be used to improve the accuracy of multiple pixels. However, using samples generated for other pixels is not always helpful: some of the sample generation algorithms implemented in Chapter 8 carefully position samples in ways that ensure good coverage of the sampling domain in a pixel. If samples from other pixels are mixed in with those, the full set of samples for a pixel may no longer have that same structure, which in turn can increase error. By not sharing samples across pixels, filter importance sampling does not have this problem.
][
  这两种方法的第一个优点是每个图像样本可以对多个像素的最终滤波值做出贡献。这对于渲染效率是有利的，因为计算图像样本辐射度所涉及的所有计算都可以用来提高多个像素的精度。然而，使用为其他像素生成的样本并不总是有帮助的：第 8 章中实现的一些样本生成算法会仔细定位样本，以确保像素采样域的良好覆盖。如果将其他像素的样本混合在一起，像素的完整样本集可能不再具有相同的结构，这反过来可能会增加误差。滤波器重要性采样通过不跨像素共享样本来避免这个问题。
]

#parec[
  Filter importance sampling has further advantages. It makes parallel rendering easier: if the renderer is parallelized in a way that has different threads working on different pixels, there is never a chance that multiple threads will need to concurrently modify the same pixel's value. A final advantage is that if there are any samples that are much brighter than the others due to a variance spike from a poorly sampled integrand, then those samples only contribute to a single pixel, rather than being smeared over multiple pixels. It is easier to fix up the resulting single-pixel artifacts than a neighborhood of them that have been affected by such a sample.
][
  滤波器重要性采样还有进一步的优势。它简化了并行渲染的过程：如果渲染器以不同线程处理不同像素的方式并行化，则永远不会有多个线程需要同时修改同一个像素值的机会。最后一个优势是，如果由于采样不佳的积分导致的方差峰值而出现任何比其他样本亮得多的样本，那么这些样本只会对单个像素做出贡献，而不是被涂抹到多个像素上。修复由此类样本影响的单个像素伪影比修复受影响的多个像素区域更容易。
]

=== The Film Interface

#parec[
  With the foundations of sensor response and pixel sample filtering established, we can introduce the Film interface. It is defined in the file base/film.h.
][
  在建立了传感器响应和像素样本滤波的基础之后，我们可以介绍 Film 接口。它在文件 base/film.h 中定义。
]
```cpp
<<Film Definition>>=
class Film : public TaggedPointer<RGBFilm, GBufferFilm, SpectralFilm> {
  public:
    <<Film Interface>>
       void AddSample(Point2i pFilm, SampledSpectrum L,
           const SampledWavelengths &lambda,
           const VisibleSurface *visibleSurface, Float weight);
       Bounds2f SampleBounds() const;
       bool UsesVisibleSurface() const;
       void AddSplat(Point2f p, SampledSpectrum v,
                     const SampledWavelengths &lambda);
       SampledWavelengths SampleWavelengths(Float u) const;
       Point2i FullResolution() const;
       Bounds2i PixelBounds() const;
       Float Diagonal() const;
       void WriteImage(ImageMetadata metadata, Float splatScale = 1);
       RGB ToOutputRGB(SampledSpectrum L, const SampledWavelengths &lambda) const;
       Image GetImage(ImageMetadata *metadata, Float splatScale = 1);
       RGB GetPixelRGB(Point2i p, Float splatScale = 1) const;
       Filter GetFilter() const;
       const PixelSensor *GetPixelSensor() const;
       std::string GetFilename() const;
       using TaggedPointer::TaggedPointer;

       static Film Create(const std::string &name,
                                const ParameterDictionary &parameters, Float exposureTime,
                                const CameraTransform &cameraTransform, Filter filter, const FileLoc *loc, Allocator alloc);

       std::string ToString() const;

};
```

#parec[
  SpectralFilm, which is not described here, records spectral images over a specified wavelength range that is discretized into non-overlapping ranges. See the documentation of pbrt's file format for more information about the SpectralFilm's use.
][
  SpectralFilm（此处未详细描述）记录了在指定波长范围内的光谱图像，该波长范围被离散化为不重叠的区间。有关 SpectralFilm 使用的更多信息，请参阅 pbrt 文件格式的文档。
]
#parec[
  Samples can be provided to the film in two ways. The first is from the Sampler selecting points on the film at which the Integrator estimates the radiance. These samples are provided to the Film via the AddSample() method, which takes the following parameters:
][
  可以通过两种方式向胶片提供样本。第一种是从 Sampler 选择胶片上的点，在这些点上 Integrator 估计辐射度。这些样本通过 Film 的 AddSample() 方法提供，该方法接受以下参数：
]

#parec[
  - The sample's pixel coordinates, pFilm. - The spectral radiance of the sample, L. - The sample's wavelengths, lambda. - An optional VisibleSurface that describes the geometry at the first visible point along the sample's camera ray. - A weight for the sample to use in computing Equation (5.13) that is returned by Filter::Sample().
][
  - 样本的像素坐标，pFilm。 - 样本的光谱辐射度，L。 - 样本的波长，lambda。 - 一个可选的 VisibleSurface，描述沿样本相机光线的第一个可见点的几何形状。 - 用于计算方程 (5.13) 的样本权重，由 Filter::Sample() 返回。
]

#parec[
  Film implementations can assume that multiple threads will not call AddSample() concurrently with the same pFilm location (though they should assume that threads will call it concurrently with different ones). Therefore, it is not necessary to worry about mutual exclusion in this method's implementation unless some data that is not unique to a pixel is modified.
][
  Film 实现可以假设多个线程不会同时调用相同 pFilm 位置的 AddSample()（尽管它们应该假设线程会同时调用不同的位置）。因此，除非修改了不是像素唯一的数据，否则在此方法的实现中无需担心互斥。
]

```cpp
<<Film Interface>>=
void AddSample(Point2i pFilm, SampledSpectrum L,
    const SampledWavelengths &lambda,
    const VisibleSurface *visibleSurface, Float weight);
```

#parec[
  The Film interface also includes a method that returns a bounding box of all the samples that may be generated. Note that this is different than the bounding box of the image pixels in the common case that the pixel filter extents are wider than a pixel.
][
  Film 接口还包括一个方法，该方法返回可能生成的所有样本的边界框。请注意，这与图像像素的边界框不同，因为在常见情况下，像素滤波器的范围比像素宽。
]

```cpp
<<Film Interface>>+=
Bounds2f SampleBounds() const;
```

#parec[
  VisibleSurface holds an assortment of information about a point on a surface.
][
  VisibleSurface 包含关于表面上某个点的信息集合。
]

```cpp
<<VisibleSurface Definition>>=
class VisibleSurface {
public:
    <<VisibleSurface Public Methods>>
    <<VisibleSurface Public Members>>
};
```


#parec[
  In addition to the point, normal, shading normal, and time, \$ \$ and \$ \$, where $x$ and y are in raster space and $z$ in camera space. These values are useful in image denoising algorithms, since they make it possible to test whether the surfaces in adjacent pixels are coplanar. The surface's albedo is its spectral distribution of reflected light under uniform illumination; this quantity can be useful for separating texture from illumination before denoising.
][
  除了点、法线、阴影法线和时间之外，\$ \$ 和 \$ \$，其中 $x$ 和 $y$ 在栅格空间中， $z$ 在相机空间中。这些值在图像去噪算法中很有用，因为它们可以测试相邻像素的表面是否共面。表面的反照率是在均匀照明下反射光的光谱分布；在去噪之前，这个量对于将纹理与照明分离很有用。
]

```cpp
<<VisibleSurface Public Members>>=
Point3f p;
Normal3f n, ns;
Point2f uv;
Float time = 0;
Vector3f dpdx, dpdy;
SampledSpectrum albedo;
```
#parec[
  We will not include the VisibleSurface constructor here, as its main function is to copy appropriate values from the SurfaceInteraction into its member variables.
][
  我们将在此省略 VisibleSurface 的构造函数，因为它的主要功能是将 SurfaceInteraction 中的适当值复制到它的成员变量中。
]

```cpp
<<VisibleSurface Public Methods>>=
VisibleSurface(const SurfaceInteraction &si, SampledSpectrum albedo,
               const SampledWavelengths &lambda);
```

#parec[
  The set member variable indicates whether a VisibleSurface has been initialized.
][
  set 成员变量指示 VisibleSurface 是否已初始化。
]

```cpp
<<VisibleSurface Public Members>>+=
bool set = false;

<<VisibleSurface Public Methods>>+=
operator bool() const { return set; }
```

#parec[
  Film implementations can indicate whether they use the VisibleSurface \* passed to their AddSample() method via UsesVisibleSurface(). Providing this information allows integrators to skip the expense of initializing a VisibleSurface if it will not be used.
][
  Film 实现可以指示它们是否使用传递给其 AddSample() 方法的 VisibleSurface \*。提供此信息允许积分器跳过初始化 VisibleSurface 的开销，如果它不会被使用的话。
]

```cpp
<<Film Interface>>+=
bool UsesVisibleSurface() const;
```

#parec[
  Light transport algorithms that sample paths starting from the light sources (such as bidirectional path require the ability to "splat" contributions to arbitrary pixels. Rather than computing the final pixel value as a weighted average of contributing splats, splats are simply summed. Generally, the more splats that are around a given pixel, the brighter the pixel will be. AddSplat() splats the provided value at the given location in the image.
][
  从光源开始采样路径的光传输算法（例如双向路径）需要能够“溅射”贡献到任意像素。与其将最终像素值计算为贡献溅射的加权平均值，溅射只是简单地相加。通常，给定像素周围的溅射越多，像素就越亮。AddSplat() 在图像中的给定位置溅射提供的值。
]

#parec[
  In contrast to AddSample(), this method may be called concurrently by multiple threads that end up updating the same pixel. Therefore, Film implementations must either implement some form of mutual exclusion or use atomic operations in their implementations of this method.
][
  与 AddSample() 相比，此方法可能会被多个线程同时调用，这些线程最终更新相同的像素。因此，Film 实现必须在此方法的实现中实现某种形式的互斥或使用原子操作。
]

```cpp
<<Film Interface>>+=
void AddSplat(Point2f p, SampledSpectrum v,
              const SampledWavelengths &lambda);
```
#parec[
  Film implementations must also provide a SampleWavelengths() method that samples from the range of wavelengths that the film's sensor responds to (e.g., using SampledWavelengths::SampleVisible()).
][
  Film 实现还必须提供 SampleWavelengths() 方法，该方法从胶片传感器响应的波长范围中采样（例如，使用 SampledWavelengths::SampleVisible()）。
]
```cpp
<<Film Interface>>+=
SampledWavelengths SampleWavelengths(Float u) const;
```

#parec[
  In addition, they must provide a handful of methods that give the extent of the image and the diagonal length of its sensor, measured in meters.
][

]

```cpp
<<Film Interface>>+=
Point2i FullResolution() const;
Bounds2i PixelBounds() const;
Float Diagonal() const;
```
#parec[
  A call to the Film::WriteImage() method directs the film to do the processing necessary to generate the final image and store it in a file. In addition to the camera transform, this method takes a scale factor that is applied to the samples provided to the AddSplat() method.
][
  调用 Film::WriteImage() 方法指示胶片进行必要的处理以生成最终图像并将其存储在文件中。除了相机变换外，此方法还采用一个比例因子，该因子应用于提供给 AddSplat() 方法的样本。
]
```cpp
<<Film Interface>>+=
void WriteImage(ImageMetadata metadata, Float splatScale = 1);
```


#parec[
  The ToOutputRGB() method allows callers to find the output RGB value that results for given spectral radiance samples from applying the PixelSensor's model, performing white balancing, and then converting to the output color space. (This method is used by the SPPMIntegrator included in the online edition, which has requirements that cause it to maintain the final image itself rather than using a Film implementation.)
][
  ToOutputRGB() 方法允许调用者找到应用 PixelSensor 模型、执行白平衡，然后转换为输出颜色空间后，给定光谱辐射样本的输出 RGB 值。（此方法由在线版中包含的 SPPMIntegrator 使用，其要求导致它自己维护最终图像，而不是使用 Film 实现。）
]
```
<<Film Interface>>+=
RGB ToOutputRGB(SampledSpectrum L, const SampledWavelengths &lambda) const;
```
A caller can also request the entire image to be returned, as well as the RGB value for a single pixel. The latter method is used for displaying in-progress images during rendering.
```
<<Film Interface>>+=
Image GetImage(ImageMetadata *metadata, Float splatScale = 1);
RGB GetPixelRGB(Point2i p, Float splatScale = 1) const;
```
#parec[
  Finally, Film implementations must provide access to a few additional values for use in other parts of the system.
][
  最后，Film 实现必须提供对系统其他部分使用的一些附加值的访问。
]

```cpp
<<Film Interface>>+=
Filter GetFilter() const;
const PixelSensor *GetPixelSensor() const;
std::string GetFilename() const;
```

=== Common Film Functionality
<common-film-functionality>
#parec[
  As we did with #link("../Cameras_and_Film/Camera_Interface.html#CameraBase")[`CameraBase`] for #link("../Cameras_and_Film/Camera_Interface.html#Camera")[`Camera`] implementations, we have written a #link("<FilmBase>")[`FilmBase`] class that #link("<Film>")[`Film`] implementations can inherit from. It collects commonly used member variables and is able to provide a few of the methods required by the #link("<Film>")[`Film`] interface.
][
  正如我们为 #link("../Cameras_and_Film/Camera_Interface.html#Camera")[`Camera`] 实现编写的 #link("../Cameras_and_Film/Camera_Interface.html#CameraBase")[`CameraBase`] 一样，我们编写了一个 #link("<FilmBase>")[`FilmBase`] 类，供 #link("<Film>")[`Film`] 实现继承。它收集了常用的成员变量，并能够提供 #link("<Film>")[`Film`] 接口所需的一些方法。
]

```cpp
class FilmBase {
  public:
    /// <<FilmBase Public Methods>>
    FilmBase(FilmBaseParameters p)
           : fullResolution(p.fullResolution), pixelBounds(p.pixelBounds),
             filter(p.filter), diagonal(p.diagonal * .001f), sensor(p.sensor),
             filename(p.filename) {
       }
       Point2i FullResolution() const { return fullResolution; }
       Bounds2i PixelBounds() const { return pixelBounds; }
       Float Diagonal() const { return diagonal; }
       Filter GetFilter() const { return filter; }
       const PixelSensor *GetPixelSensor() const { return sensor; }
       std::string GetFilename() const { return filename; }
       SampledWavelengths SampleWavelengths(Float u) const {
           return SampledWavelengths::SampleVisible(u);
       }
       Bounds2f SampleBounds() const;
       std::string BaseToString() const;
  protected:
    <<FilmBase Protected Members>>       Point2i fullResolution;
       Bounds2i pixelBounds;
       Filter filter;
       Float diagonal;
       const PixelSensor *sensor;
       std::string filename;
};
```

#parec[
  The #link("<FilmBase>")[`FilmBase`] constructor takes a number of values: the overall resolution of the image in pixels; a bounding box that may specify a subset of the full image; a filter function; a #link("<PixelSensor>")[`PixelSensor`];; the length of the diagonal of the film's physical area; and the filename for the output image. These are all bundled up into a small structure in order to shorten the parameter lists of forthcoming constructors.
][
  #link("<FilmBase>")[`FilmBase`] 构造函数接受多个值：图像的整体分辨率（以像素为单位）；可能指定完整图像子集的包围盒；滤波器函数；#link("<PixelSensor>")[`PixelSensor`];；胶片物理区域对角线的长度；以及输出图像文件的名称。这些都被打包成一个小结构体，以缩短即将到来的构造函数的参数列表。
]

```cpp
struct FilmBaseParameters {
    Point2i fullResolution;
    Bounds2i pixelBounds;
    Filter filter;
    Float diagonal;
    const PixelSensor *sensor;
    std::string filename;
};
```
#parec[
  The #link("<FilmBase>")[`FilmBase`] constructor then just copies the various values from the parameter structure, converting the film diagonal length from millimeters (as specified in scene description files) to meters, the unit used for measuring distance in `pbrt`.
][
  #link("<FilmBase>")[`FilmBase`] 构造函数然后只是从参数结构体中复制各种值，将胶片对角线长度从毫米（如场景描述文件中指定的）转换为米，这是 `pbrt` 中用于测量距离的单位。
]

```cpp
FilmBase(FilmBaseParameters p)
    : fullResolution(p.fullResolution), pixelBounds(p.pixelBounds),
      filter(p.filter), diagonal(p.diagonal * .001f), sensor(p.sensor),
      filename(p.filename) {
}
```

```cpp
Point2i fullResolution;
Bounds2i pixelBounds;
Filter filter;
Float diagonal;
const PixelSensor *sensor;
std::string filename;
```

#parec[
  Having these values makes it possible to immediately implement a number of the methods required by the #link("<Film>")[`Film`] interface.
][
  拥有这些值可以直接实现 #link("<Film>")[`Film`] 接口所需的许多方法。
]

```cpp
Point2i FullResolution() const { return fullResolution; }
Bounds2i PixelBounds() const { return pixelBounds; }
Float Diagonal() const { return diagonal; }
Filter GetFilter() const { return filter; }
const PixelSensor *GetPixelSensor() const { return sensor; }
std::string GetFilename() const { return filename; }
```
#parec[
  An implementation of `SampleWavelengths()` samples according to the distribution in Equation $5.9$.
][
  `SampleWavelengths()` 的实现根据方程 $5.9$ 中的分布进行采样。
]

```cpp
SampledWavelengths SampleWavelengths(Float u) const {
    return SampledWavelengths::SampleVisible(u);
}
```
#parec[
  The #link("<Film::SampleBounds>")[`Film::SampleBounds()`] method can also be easily implemented, given the #link("../Sampling_and_Reconstruction/Image_Reconstruction.html#Filter")[`Filter`];. Computing the sample bounds involves both expanding by the filter radius and accounting for half-pixel offsets that come from the conventions used in `pbrt` for pixel coordinates; these are explained in more detail in Section 8.1.4.
][
  #link("<Film::SampleBounds>")[`Film::SampleBounds()`] 方法也可以很容易地实现，给定 #link("../Sampling_and_Reconstruction/Image_Reconstruction.html#Filter")[`Filter`];。计算采样边界涉及到通过滤波器半径扩展并考虑在 `pbrt` 中用于像素坐标的约定的半像素偏移；这些在第 8.1.4 节中有更详细的解释。
]

```cpp
Bounds2f FilmBase::SampleBounds() const {
    Vector2f radius = filter.Radius();
    return Bounds2f(pixelBounds.pMin - radius + Vector2f(0.5f, 0.5f),
                    pixelBounds.pMax + radius - Vector2f(0.5f, 0.5f));
}
```



=== RGBFilm
<rgbfilm>
#parec[
  #link("<RGBFilm>")[`RGBFilm`] records an image represented by RGB color.
][
  #link("<RGBFilm>")[`RGBFilm`] 记录了一个由 RGB 颜色表示的图像。
]

```cpp
class RGBFilm : public FilmBase {
  public:
    <<RGBFilm Public Methods>>
    bool UsesVisibleSurface() const { return false; }
    void AddSample(Point2i pFilm, SampledSpectrum L,
        const SampledWavelengths &lambda,
        const VisibleSurface *, Float weight) {
           <<Convert sample radiance to PixelSensor RGB>>
           RGB rgb = sensor->ToSensorRGB(L, lambda);
           <<Optionally clamp sensor RGB value>>
           Float m = std::max({rgb.r, rgb.g, rgb.b});
              if (m > maxComponentValue)
                  rgb *= maxComponentValue / m;
           <<Update pixel values with filtered sample contribution>>
           Pixel &pixel = pixels[pFilm];
              for (int c = 0; c < 3; ++c)
                  pixel.rgbSum[c] += weight * rgb[c];
              pixel.weightSum += weight;
       }
       RGB GetPixelRGB(Point2i p, Float splatScale = 1) const {
           const Pixel &pixel = pixels[p];
           RGB rgb(pixel.rgbSum[0], pixel.rgbSum[1], pixel.rgbSum[2]);
           <<Normalize rgb with weight sum>>
           Float weightSum = pixel.weightSum;
              if (weightSum != 0)
                  rgb /= weightSum;
           <<Add splat value at pixel>>
           for (int c = 0; c < 3; ++c)
               rgb[c] += splatScale * pixel.rgbSplat[c] / filterIntegral;
           <<Convert rgb to output RGB color space>>
           rgb = outputRGBFromSensorRGB * rgb;
           return rgb;
       }
       RGBFilm(FilmBaseParameters p, const RGBColorSpace *colorSpace, Float maxComponentValue = Infinity,
               bool writeFP16 = true, Allocator alloc = {});

       static RGBFilm *Create(const ParameterDictionary &parameters, Float exposureTime, Filter filter,
                              const RGBColorSpace *colorSpace, const FileLoc *loc,
                              Allocator alloc);

       PBRT_CPU_GPU
       void AddSplat(Point2f p, SampledSpectrum v, const SampledWavelengths &lambda);

       void WriteImage(ImageMetadata metadata, Float splatScale = 1);
       Image GetImage(ImageMetadata *metadata, Float splatScale = 1);

       std::string ToString() const;
       RGB ToOutputRGB(SampledSpectrum L, const SampledWavelengths &lambda) const {
           RGB sensorRGB = sensor->ToSensorRGB(L, lambda);
           return outputRGBFromSensorRGB * sensorRGB;
       }
  private:
    <<RGBFilm::Pixel Definition>>       struct Pixel {
           double rgbSum[3] = {0., 0., 0.};
           double weightSum = 0.;
           AtomicDouble rgbSplat[3];
       };
    <<RGBFilm Private Members>>
    const RGBColorSpace *colorSpace;
       Float maxComponentValue;
       bool writeFP16;
       Float filterIntegral;
       SquareMatrix<3> outputRGBFromSensorRGB;
       Array2D<Pixel> pixels;
};
```

#parec[
  In addition to the parameters that are passed along to #link("<FilmBase>")[`FilmBase`];, #link("<RGBFilm>")[`RGBFilm`] takes a color space to use for the output image, a parameter that allows specifying the maximum value of an RGB color component, and a parameter that controls the floating-point precision in the output image.
][
  除了传递给 #link("<FilmBase>")[`FilmBase`] 的参数外， #link("<RGBFilm>")[`RGBFilm`] 还需要一个用于输出图像的颜色空间，一个允许指定 RGB 颜色分量最大值的参数，以及一个控制输出图像浮点精度的参数。
]

```cpp
RGBFilm::RGBFilm(FilmBaseParameters p, const RGBColorSpace *colorSpace,
                 Float maxComponentValue, bool writeFP16, Allocator alloc)
    : FilmBase(p), pixels(p.pixelBounds, alloc), colorSpace(colorSpace),
      maxComponentValue(maxComponentValue), writeFP16(writeFP16) {
    filterIntegral = filter.Integral();
    <<Compute outputRGBFromSensorRGB matrix>>
    outputRGBFromSensorRGB = colorSpace->RGBFromXYZ *
           sensor->XYZFromSensorRGB;
}
```
#parec[
  The integral of the filter function will be useful to normalize the filter values used for samples provided via `AddSplat()`, so it is cached in a member variable.
][
  滤波器函数的积分对归一化通过 `AddSplat()` 提供的样本的滤波值很有用，因此它被缓存到一个成员变量中。
]

```cpp
const RGBColorSpace *colorSpace;
Float maxComponentValue;
bool writeFP16;
Float filterIntegral;
```


#parec[
  The color space for the final image is given by a user-specified #link("../Radiometry,_Spectra,_and_Color/Color.html#RGBColorSpace")[RGBColorSpace] that is unlikely to be the same as the sensor's RGB color space. The constructor therefore computes a $3 times 3$ matrix that transforms sensor RGB values to the output color space.
][
  最终图像的颜色空间由用户指定的 #link("../Radiometry,_Spectra,_and_Color/Color.html#RGBColorSpace")[RGBColorSpace] 决定，它通常不会与传感器的 RGB 颜色空间相同。因此，构造函数计算了一个 $3 times 3$ 的矩阵，该矩阵将传感器 RGB 值转换为输出颜色空间。
]

```cpp
outputRGBFromSensorRGB = colorSpace->RGBFromXYZ *
    sensor->XYZFromSensorRGB;
```

#parec[
  Given the pixel resolution of the (possibly cropped) image, the constructor allocates a 2D array of `Pixel` structures, with one for each pixel. The running weighted sums of pixel contributions are represented using RGB colors in the `rgbSum` member variable. `weightSum` holds the sum of filter weight values for the sample contributions to the pixel. These respectively correspond to the numerator and denominator in Equation (5.13). Finally, `rgbSplat` holds an (unweighted) sum of sample splats.
][
  根据（可能被裁剪的）图像的像素分辨率，构造函数为每个像素分配了一个 `Pixel` 结构的二维数组。像素贡献的加权和使用 `rgbSum` 成员变量中的 RGB 颜色表示。`weightSum` 保存滤波器权重值的和，这些值对应于像素样本贡献的分子和分母，分别与方程（5.13）中的分子和分母相对应。最后，`rgbSplat` 保存样本点的（未加权）和。
]

#parec[
  Double-precision floating point is used for all of these quantities. Single-precision `float`s are almost always sufficient, but when used for reference images rendered with high sample counts they may have insufficient precision to accurately store their associated sums. Although it is rare for this error to be visually evident, it can cause problems with reference images that are used to evaluate the error of Monte Carlo sampling algorithms.
][
  所有这些数量都使用双精度浮点数。单精度 `float` 几乎总是足够的，但在使用高样本数渲染的参考图像时，它们可能没有足够的精度来准确存储其相关的和。尽管这种错误在视觉上很少明显，但它可能会对用于评估蒙特卡罗采样算法误差的参考图像造成问题。
]

#parec[
  Figure 5.24 shows an example of this problem. We rendered a reference image of a test scene using 4 million samples in each pixel, using both 32-bit and 64-bit floating-point values for the [RGBFilm](`#RGBFilm`) pixel values. We then plotted mean squared error (MSE) as a function of sample count. For an unbiased Monte Carlo estimator, MSE is $O(1/n)$ in the number of samples taken $n$ ; on a log-log plot, it should be a straight line with slope $-1$. However, we can see that for $n > 1000$ with a 32-bit float reference image, the reduction in MSE seems to flatten out—more samples do not seem to reduce error. With 64-bit floats, the curve maintains its expected path.
][
  图 5.24 展示了这个问题的一个示例。我们渲染了一个测试场景的参考图像，每个像素使用 400 万个样本，并分别使用 32 位和 64 位浮点值表示 [RGBFilm](`#RGBFilm`) 像素值。然后我们将均方误差（MSE）绘制为样本数量的函数。对于无偏的蒙特卡罗估计器，MSE 随着样本数量 $n$ 的变化是 $O(1\/n)$ ；在对数-对数图中，它应当是一条斜率为 $-1$ 的直线。然而，我们可以看到，当 $n > 1000$ 时，32 位浮点参考图像的 MSE 减少似乎趋于平坦——更多的样本并未显著减少误差。使用 64 位浮点时，曲线保持了预期的路径。
]


```cpp
// <<RGBFilm::Pixel Definition>>=
struct Pixel {
    double rgbSum[3] = {0., 0., 0.};
    double weightSum = 0.;
    AtomicDouble rgbSplat[3];
};
```

#parec[
  The [RGBFilm](`#RGBFilm`) does not use the `VisibleSurface *` passed to `AddSample()`.
][
  [RGBFilm](`#RGBFilm`) 不使用传递给 `AddSample()` 的 `VisibleSurface *`。
]

```cpp
bool UsesVisibleSurface() const { return false; }
```

#parec[
  `AddSample()` converts spectral radiance to sensor RGB before updating the `Pixel` corresponding to the point `pFilm`.
][
  `AddSample()` 在更新与 `pFilm` 对应的 `Pixel` 之前，将光谱辐射转换为传感器 RGB。
]

```cpp
RGB rgb = sensor->ToSensorRGB(L, lambda);
```

#parec[
  Images rendered with Monte Carlo integration can exhibit bright spikes of noise in pixels where the sampling distributions that were used do not match the integrand well such that when $f(x)\/p(x)$ is computed in the Monte Carlo estimator, $f(x)$ is very large and $p(x)$ is very small. (Such pixels are colloquially called “fireflies.”) Many additional samples may be required to get an accurate estimate for that pixel.
][
  使用蒙特卡罗积分渲染的图像可能会在像素中出现明亮的噪点，即所谓的“萤火虫”。这些噪点出现在采样分布与被积函数不匹配的情况下，当在蒙特卡罗估计中计算 $f(x) \/p(x)$ 时， $f(x)$ 非常大而 $p(x)$ 非常小。可能需要许多额外的样本才能为这些像素获得准确的估计。
]

#parec[
  A widely used technique to reduce the effect of fireflies is to clamp all sample contributions to some maximum amount. Doing so introduces error: energy is lost, and the image is no longer an unbiased estimate of the true image. However, when the aesthetics of rendered images are more important than their mathematics, this can be a useful remedy. Figure 5.25 shows an example of its use.
][
  减少“萤火虫”效果的一个常用技术是将所有样本贡献限制在某个最大值。这样做会引入误差：能量丢失，图像不再是对真实图像的无偏估计。然而，当渲染图像的美学比其数学属性更为重要时，这可能是一个有用的补救措施。图 5.25 显示了该技术的一个示例。
]

#parec[
  The [RGBFilm](`RGBFilm`)'s `maxComponentValue` parameter can be set to a threshold that is used for clamping. It is infinite by default, and no clamping is performed.
][
  [RGBFilm](`RGBFilm`) 的 `maxComponentValue` 参数可以设置为用于限制的阈值。默认情况下，它是无限的，不会执行任何限制。
]

```cpp
Float m = std::max({rgb.r, rgb.g, rgb.b});
if (m > maxComponentValue)
    rgb *= maxComponentValue / m;
```

#parec[
  Given the possibly clamped RGB value, the pixel it lies in can be updated by adding its contributions to the running sums of the numerator and denominator of Equation (5.13).
][
  在得到可能受限的 RGB 值后，可以通过将其贡献添加到方程（5.13）的分子和分母的累积和中来更新其所在的像素。
]

```cpp
Pixel &pixel = pixels[pFilm];
for (int c = 0; c < 3; ++c)
    pixel.rgbSum[c] += weight * rgb[c];
pixel.weightSum += weight;
```

#parec[
  The `AddSplat()` method first reuses the first two fragments from `AddSample()` to compute the RGB value of the provided radiance `L`.
][
  `AddSplat()` 方法首先重用 `AddSample()` 的前两个片段来计算提供的辐射 `L` 的 RGB 值。
]

```cpp
void RGBFilm::AddSplat(Point2f p, SampledSpectrum L,
                       const SampledWavelengths &lambda) {
    <<Convert sample radiance to PixelSensor RGB>>       RGB rgb = sensor->ToSensorRGB(L, lambda);
    <<Optionally clamp sensor RGB value>>       Float m = std::max({rgb.r, rgb.g, rgb.b});
       if (m > maxComponentValue)
           rgb *= maxComponentValue / m;
    <<Compute bounds of affected pixels for splat, splatBounds>>       Point2f pDiscrete = p + Vector2f(0.5, 0.5);
       Vector2f radius = filter.Radius();
       Bounds2i splatBounds(Point2i(Floor(pDiscrete - radius)),
                            Point2i(Floor(pDiscrete + radius)) + Vector2i(1, 1));
       splatBounds = Intersect(splatBounds, pixelBounds);
    for (Point2i pi : splatBounds) {
        <<Evaluate filter at pi and add splat contribution>>           Float wt = filter.Evaluate(Point2f(p - pi - Vector2f(0.5, 0.5)));
           if (wt != 0) {
               Pixel &pixel = pixels[pi];
               for (int i = 0; i < 3; ++i)
                   pixel.rgbSplat[i].Add(wt * rgb[i]);
           }
    }
}
```


#parec[
  Because splatted contributions are not a result of pixel samples but are points in the scene that are projected onto the film plane, it is necessary to consider their contribution to multiple pixels, since each pixel's reconstruction filter generally extends out to include contributions from nearby pixels.
][
  由于点溅的贡献不是像素样本的结果，而是场景中的点投影到胶片平面上的结果，因此有必要考虑它们对多个像素的贡献，因为每个像素的重建滤波器通常会扩展以包括来自邻近像素的贡献。
]

#parec[
  First, a bounding box of potentially affected pixels is found using the filter's radius. See Section 8.1.4, which explains the conventions for indexing into pixels in `pbrt` and, in particular, the addition of $(0.5, 0.5)$ to the pixel coordinate here.
][
  首先，使用滤波器的半径找到可能受影响像素的边界框。参见第 8.1.4 节，其中解释了 `pbrt` 中像素索引的约定，特别是这里在像素坐标上添加 $(0.5, 0.5)$ 的原因。
]

```cpp
Point2f pDiscrete = p + Vector2f(0.5, 0.5);
Vector2f radius = filter.Radius();
Bounds2i splatBounds(Point2i(Floor(pDiscrete - radius)),
                     Point2i(Floor(pDiscrete + radius)) + Vector2i(1, 1));
splatBounds = Intersect(splatBounds, pixelBounds);
```

#parec[
  If the filter weight is nonzero, the splat's weighted contribution is added. Unlike with `AddSample()`, no sum of filter weights is maintained; normalization is handled later using the filter's integral, as per Equation (5.10).
][
  如果滤波器权重非零，则添加样本点的加权贡献。与 `AddSample()` 不同，不维护滤波器权重的总和；归一化稍后使用滤波器的积分处理，正如方程（5.10）所述。
]

```cpp
Float wt = filter.Evaluate(Point2f(p - pi - Vector2f(0.5, 0.5)));
if (wt != 0) {
    Pixel &pixel = pixels[pi];
    for (int i = 0; i < 3; ++i)
        pixel.rgbSplat[i].Add(wt * rgb[i]);
}
```

#parec[
  `GetPixelRGB()` returns the final `RGB` value for a given pixel in the [RGBFilm](`RGBFilm`)'s output color space.
][
  `GetPixelRGB()` 返回 [RGBFilm](`RGBFilm`) 输出颜色空间中某个像素的最终 `RGB` 值。
]

```cpp
RGB GetPixelRGB(Point2i p, Float splatScale = 1) const {
    const Pixel &pixel = pixels[p];
    RGB rgb(pixel.rgbSum[0], pixel.rgbSum[1], pixel.rgbSum[2]);
    <<Normalize rgb with weight sum>>       Float weightSum = pixel.weightSum;
       if (weightSum != 0)
           rgb /= weightSum;
    <<Add splat value at pixel>>       for (int c = 0; c < 3; ++c)
           rgb[c] += splatScale * pixel.rgbSplat[c] / filterIntegral;
    <<Convert rgb to output RGB color space>>       rgb = outputRGBFromSensorRGB * rgb;
    return rgb;
}
```

#parec[
  First, the final pixel contribution from the values provided by `AddSample()` is computed via Equation (5.13).
][
  首先，`AddSample()` 提供的值通过方程（5.13）计算出最终的像素贡献。
]

```cpp
Float weightSum = pixel.weightSum;
if (weightSum != 0)
    rgb /= weightSum;
```

#parec[
  Then Equation (5.10) can be applied to incorporate any splatted values.
][
  然后可以应用方程（5.10）来整合所有的样本点值。
]

```cpp
for (int c = 0; c < 3; ++c)
    rgb[c] += splatScale * pixel.rgbSplat[c] / filterIntegral;
```

#parec[
  Finally, the color conversion matrix brings the RGB value into the output color space.
][
  最后，颜色转换矩阵将 RGB 值转换为输出颜色空间。
]

```cpp
rgb = outputRGBFromSensorRGB * rgb;
```

#parec[
  `ToOutputRGB()`'s implementation first uses the sensor to compute a sensor RGB and then converts to the output color space.
][
  `ToOutputRGB()` 的实现首先使用传感器计算传感器 RGB 值，然后转换为输出颜色空间。
]

```cpp
RGB ToOutputRGB(SampledSpectrum L, const SampledWavelengths &lambda) const {
    RGB sensorRGB = sensor->ToSensorRGB(L, lambda);
    return outputRGBFromSensorRGB * sensorRGB;
}
```


#parec[
  We will not include the straightforward `RGBFilm` `WriteImage()` or `GetImage()` method implementations in the book. The former calls `GetImage()` before calling `Image::Write()`, and the latter fills in an image using `GetPixelRGB()` to get each pixel's value.
][
  我们不会在书中包含简单的 `RGBFilm` `WriteImage()` 或 `GetImage()` 方法的实现。前者在调用 `Image::Write()` 之前调用 `GetImage()`，后者则使用 `GetPixelRGB()` 获取每个像素的值以填充图像。
]


=== GBufferFilm

#parec[
  The `GBufferFilm` stores not only RGB at each pixel, but also additional information about the geometry at the first visible intersection point. This additional information is useful for a variety of applications, ranging from image denoising algorithms to providing training data for machine learning applications.
][
  `GBufferFilm`不仅在每个像素存储RGB，还存储关于第一个可见的交点的额外几何信息。这些额外的信息对于各种应用非常有用，从图像去噪算法到为机器学习应用程序提供训练数据。
]



```cpp
class GBufferFilm : public FilmBase {
  public:
    <<GBufferFilm Public Methods>>
    GBufferFilm(FilmBaseParameters p, const AnimatedTransform &outputFromRender,
                                      bool applyInverse, const RGBColorSpace *colorSpace,
                   Float maxComponentValue = Infinity, bool writeFP16 = true,
                   Allocator alloc = {});

       static GBufferFilm *Create(const ParameterDictionary &parameters, Float
                                  exposureTime, const CameraTransform &cameraTransform, Filter filter,
                                  const RGBColorSpace *colorSpace, const FileLoc *loc,
                                  Allocator alloc);

       PBRT_CPU_GPU
       void AddSample(Point2i pFilm, SampledSpectrum L,
                      const SampledWavelengths &lambda,
                      const VisibleSurface *visibleSurface, Float weight);

       PBRT_CPU_GPU
       void AddSplat(Point2f p, SampledSpectrum v, const SampledWavelengths &lambda);

       PBRT_CPU_GPU
       RGB ToOutputRGB(SampledSpectrum L, const SampledWavelengths &lambda) const {
           RGB cameraRGB = sensor->ToSensorRGB(L, lambda);
           return outputRGBFromSensorRGB * cameraRGB;
       }

       PBRT_CPU_GPU
       bool UsesVisibleSurface() const { return true; }

       PBRT_CPU_GPU
       RGB GetPixelRGB(Point2i p, Float splatScale = 1) const {
           const Pixel &pixel = pixels[p];
           RGB rgb(pixel.rgbSum[0], pixel.rgbSum[1], pixel.rgbSum[2]);

           // Normalize pixel with weight sum
           Float weightSum = pixel.weightSum;
           if (weightSum != 0)
               rgb /= weightSum;

           // Add splat value at pixel
           for (int c = 0; c < 3; ++c)
               rgb[c] += splatScale * pixel.rgbSplat[c] / filterIntegral;

           rgb = outputRGBFromSensorRGB * rgb;

           return rgb;
       }

       void WriteImage(ImageMetadata metadata, Float splatScale = 1);
       Image GetImage(ImageMetadata *metadata, Float splatScale = 1);

       std::string ToString() const;
  private:
    <<GBufferFilm::Pixel Definition>>
    struct Pixel {
           double rgbSum[3] = {0., 0., 0.};
           double weightSum = 0., gBufferWeightSum = 0.;
           AtomicDouble rgbSplat[3];
           Point3f pSum;
           Float dzdxSum = 0, dzdySum = 0;
           Normal3f nSum, nsSum;
           Point2f uvSum;
           double rgbAlbedoSum[3] = {0., 0., 0.};
           VarianceEstimator<Float> rgbVariance[3];
       };
    <<GBufferFilm Private Members>>
    AnimatedTransform outputFromRender;
       bool applyInverse;
       Array2D<Pixel> pixels;
       const RGBColorSpace *colorSpace;
       Float maxComponentValue;
       bool writeFP16;
       Float filterIntegral;
       SquareMatrix<3> outputRGBFromSensorRGB;
};
```

#parec[
  We will not include any of the `GBufferFilm` implementation other than its `Pixel` structure, which augments the one used in #link("<RGBFilm>")[RGBFilm] with additional fields that store geometric information. It also stores estimates of the variance of the red, green, and blue color values at each pixel using the #link("../Utilities/Mathematical_Infrastructure.html#VarianceEstimator")[VarianceEstimator] class, which is defined in Section #link("../Utilities/Mathematical_Infrastructure.html#sec:robust-variance-estimation")[B.2.11];. The rest of the implementation is a straightforward generalization of #link("<RGBFilm>")[RGBFilm] that also updates these additional values.
][
  我们将不包括任何`GBufferFilm`的实现，除了其`Pixel`结构，该结构通过附加字段存储几何信息来增强#link("<RGBFilm>")[RGBFilm];中使用的结构。它还使用#link("../Utilities/Mathematical_Infrastructure.html#VarianceEstimator")[VarianceEstimator];类（在#link("../Utilities/Mathematical_Infrastructure.html#sec:robust-variance-estimation")[第B.2.11节];中定义）存储每个像素红、绿、蓝颜色值的方差估计。其余的实现是#link("<RGBFilm>")[RGBFilm];的直接推广，也更新了这些附加值。
]



```cpp
struct Pixel {
    double rgbSum[3] = {0., 0., 0.};
    double weightSum = 0., gBufferWeightSum = 0.;
    AtomicDouble rgbSplat[3];
    Point3f pSum;
    Float dzdxSum = 0, dzdySum = 0;
    Normal3f nSum, nsSum;
    Point2f uvSum;
    double rgbAlbedoSum[3] = {0., 0., 0.};
    VarianceEstimator<Float> rgbVariance[3];
};
```


